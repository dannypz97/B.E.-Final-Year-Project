import re
import sys
import numpy as np
from sklearn.preprocessing import  LabelBinarizer
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.layers import Dense, Input, Reshape, Concatenate, Flatten
from keras.layers import Conv2D, MaxPool2D, Embedding, Dropout
from keras.models import Model

class SentenceClassifier:
    def __init__(self):
        self.MAX_SEQUENCE_LENGTH = 51
        self.EMBEDDING_DIM = 50
        self.LABEL_COUNT = 0
        self.WORD_INDEX = dict()
        self.LABEL_ENCODER = None

    def clean_str(self, string):
        """
        Cleans each string and convert to lower case.
        """
        string = re.sub(r"[^A-Za-z0-9:(),!?\'\`]", " ", string)
        string = re.sub(r" : ", ":", string)
        string = re.sub(r"\'s", " \'s", string)
        string = re.sub(r"\'ve", " \'ve", string)
        string = re.sub(r"n\'t", " n\'t", string)
        string = re.sub(r"\'re", " \'re", string)
        string = re.sub(r"\'d", " \'d", string)
        string = re.sub(r"\'ll", " \'ll", string)
        string = re.sub(r",", " , ", string)
        string = re.sub(r"!", " ! ", string)
        string = re.sub(r"\(", " \( ", string)
        string = re.sub(r"\)", " \) ", string)
        string = re.sub(r"\?", " \? ", string)
        string = re.sub(r"\s{2,}", " ", string)
        return string.strip().lower()

    def load_data_train(self, DATA_PATH = './data/trec/train_5500.label.txt'):
        """
        Reads the training dataset from data directory and splits into sentences (X) and labels (Y).
        """
        dataset = list(open(DATA_PATH, encoding='utf-8', errors='replace').readlines())
        dataset = [self.clean_str(sent) for sent in dataset]
        y_train = [s.split(' ')[0].split(':')[0] for s in dataset]
        x_train = [s.split(" ")[1:] for s in dataset]
        return x_train, y_train

    def load_test_data(self, DATA_PATH = './data/trec/TREC_10.label.txt'):
        """
        Reads the test dataset from data directory and splits into sentences (X) and labels (Y).
        """
        dataset = list(open(DATA_PATH, encoding='utf-8', errors='replace').readlines())
        dataset = [self.clean_str(sent) for sent in dataset]
        y_test = [s.split(' ')[0].split(':')[0] for s in dataset]
        x_test = [s.split(" ")[1:] for s in dataset]
        return x_test, y_test

    def encode_train(self, x_train, y_train):
        """
        Encodes each sentence into integer representation and pads all sentences to same length
        """
        tokenizer = Tokenizer(lower=True, char_level=False)
        tokenizer.fit_on_texts(x_train)
        self.WORD_INDEX = tokenizer.word_index
        x_train_encoded = tokenizer.texts_to_sequences(x_train)
        x_train_encoded_padded = pad_sequences(x_train_encoded, maxlen=self.MAX_SEQUENCE_LENGTH, padding='post')

        encoder = LabelBinarizer()
        encoder.fit(y_train)
        self.LABEL_ENCODER = encoder #Called again to transfrom y_test.

        y_train_one_hot = encoder.fit_transform(y_train)
        self.LABEL_COUNT = y_train_one_hot.shape[1]
        print("\tUnique Tokens in Training Data: ", len(self.WORD_INDEX))
        return x_train_encoded_padded, y_train_one_hot

    def encode_test(self, x_test, y_test):

        x_test_encoded = list()
        for sentence in x_test:
            x_test = [self.WORD_INDEX[w] for w in sentence if w in self.WORD_INDEX]
            x_test_encoded.append(x_test)
        x_test_encoded_padded = pad_sequences(x_test_encoded, maxlen=self.MAX_SEQUENCE_LENGTH, padding='post')

        y_test_one_hot = self.LABEL_ENCODER.transform(y_test)

        return x_test_encoded_padded, y_test_one_hot

    def train_dev_split(self, X_train_encoded_padded, Y_train_one_hot):
        """
        Splits the training data set into training and validation(dev) set based on the value specified by
        the parameter VALIDATION_SPLIT.
        :param X_train_encoded_padded: Training sentences which are integer encoded and padded.
        :param Y_train_one_hot: Labels for training sentences in one-hot format.
        :return: Training sentences (x_train) and corresponding labels(y_train). Validation sentences (x_val) and
        corresponding labels (y_val).
        """
        indices = np.arange(X_train_encoded_padded.shape[0])
        np.random.shuffle(indices)
        X_train_encoded_padded = X_train_encoded_padded[indices]
        Y_train_one_hot = Y_train_one_hot[indices]
        num_validation_samples = int(self.VALIDATION_SPLIT * X_train_encoded_padded.shape[0])
        x_train = X_train_encoded_padded[:-num_validation_samples]
        y_train = Y_train_one_hot[:-num_validation_samples]
        x_val = X_train_encoded_padded[-num_validation_samples:]
        y_val = Y_train_one_hot[-num_validation_samples:]
        return x_train, y_train, x_val, y_val

    def load_embeddings(self, EMBED_PATH='./embeddings/glove.6B.50d.txt'):
        """
        Load pre-trained embeddings into memory.
        """
        embeddings_index = {}
        try:
        	f = open(EMBED_PATH, encoding='utf-8')
        except FileNotFoundError:
        	print("Embeddings missing.")
        	sys.exit()
        for line in f:
            values = line.rstrip().rsplit(' ')
            word = values[0]
            vec = np.asarray(values[1:], dtype='float32')
            embeddings_index[word] = vec
        f.close()
        print("\tNumber of tokens in embeddings file: ", len(embeddings_index))
        return embeddings_index

    def create_embedding_matrix(self, embeddings_index):
        """
        Creates an embedding matrix for all the words(vocab) in the training data with shape (vocab, EMBEDDING_DIM).
        Out-of-vocab words will be randomly initialized to values between +0.25 and -0.25.
        """
        words_not_found = []
        vocab = len(self.WORD_INDEX) + 1
        embedding_matrix = np.random.uniform(-0.25, 0.25, size=(vocab, self.EMBEDDING_DIM))
        for word, i in self.WORD_INDEX.items():
            if i >= vocab:
                continue
            embedding_vector = embeddings_index.get(word)
            if (embedding_vector is not None) and len(embedding_vector) > 0:
                embedding_matrix[i] = embedding_vector
            else:
                words_not_found.append(word)
        # print('Number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))
        print("\tShape of embedding matrix: ", str(embedding_matrix.shape))
        print("\tNo. of words not found in pre-trained embeddings: ", len(words_not_found))
        return embedding_matrix

    def sentence_classifier_cnn(self, embedding_matrix):
        """
        A static CNN model.
        Makes uses of Keras functional API for constructing the model.
        """
        inputs = Input(shape=(self.MAX_SEQUENCE_LENGTH,), dtype='int32')
        X_input = Embedding(input_dim=(len(self.WORD_INDEX) + 1), output_dim=self.EMBEDDING_DIM,
                            weights=[embedding_matrix],
                            input_length=self.MAX_SEQUENCE_LENGTH, trainable=False)(inputs)

        X_input = Reshape((self.MAX_SEQUENCE_LENGTH, self.EMBEDDING_DIM, 1))(X_input)
        # print(X_input)

        X1 = Conv2D(128, kernel_size=(3, self.EMBEDDING_DIM), padding='valid', kernel_initializer='normal',
                    activation='relu',
                    name='conv1Filter1')(X_input)
        maxpool_1 = MaxPool2D(pool_size=(48, 1), strides=(1, 1), padding='valid', name='maxpool1')(X1)

        X2 = Conv2D(128, kernel_size=(4, self.EMBEDDING_DIM), padding='valid', kernel_initializer='normal',
                    activation='relu',
                    name='conv1Filter2')(X_input)
        maxpool_2 = MaxPool2D(pool_size=(47, 1), strides=(1, 1), padding='valid', name='maxpool2')(X2)

        X3 = Conv2D(128, kernel_size=(5, self.EMBEDDING_DIM), padding='valid', kernel_initializer='normal',
                    activation='relu',
                    name='conv1Filter3')(X_input)
        maxpool_3 = MaxPool2D(pool_size=(46, 1), strides=(1, 1), padding='valid', name='maxpool3')(X3)

        concatenated_tensor = Concatenate(axis=1)([maxpool_1, maxpool_2, maxpool_3])

        flatten = Flatten()(concatenated_tensor)
        dropout = Dropout(0.5)(flatten)

        hidden = Dense(units=500, activation="relu")(dropout)
        output = Dense(units=self.LABEL_COUNT, activation='sigmoid', name='fully_connected_affine_layer')(hidden)

        model = Model(inputs=inputs, outputs=output, name='intent_classifier')
        print("Model Summary")
        print(model.summary())

        model.compile(loss='binary_crossentropy',
                      optimizer='adam',
                      metrics=['accuracy'])
        model.fit(x_train_encoded_padded, y_train_one_hot,
                  batch_size=64,
                  epochs=20,
                  verbose=2)


        return model


if __name__ == '__main__':
    classifier = SentenceClassifier()

    print("Loading Training Data set...")
    x_train, y_train = classifier.load_data_train()
    print("Encoding Training Data set...")
    x_train_encoded_padded, y_train_one_hot = classifier.encode_train(x_train, y_train)

    print("Loading embedding vectors...")
    embeddings_index = classifier.load_embeddings()
    print("Generating Embedding Matrix...")
    embedding_matrix = classifier.create_embedding_matrix(embeddings_index)

    print("Loading Test Data set...")
    x_test, y_test = classifier.load_test_data()

    print("Encoding Test Data set...")
    x_test_encoded_padded, y_test_one_hot = classifier.encode_test(x_test, y_test)

    print("Training Started...")

    model = classifier.sentence_classifier_cnn(embedding_matrix)

    print("Evaluating the model on the Test Data set...")
    scores = model.evaluate(x_test_encoded_padded, y_test_one_hot, verbose=2)
    print("%s: %.2f%%" % (model.metrics_names[1], scores[1] * 100))

    sentence = ["What is the full form of WWW and who is al-Majid and where is Syria?"]
    sentence = classifier.clean_str(sentence[0])
    sentence_encoded = [[classifier.WORD_INDEX[w] for w in sentence.split(' ') if w in classifier.WORD_INDEX]]

    sentence_encoded_padded = pad_sequences(sentence_encoded, maxlen=classifier.MAX_SEQUENCE_LENGTH, padding='post')
    print(model.predict(sentence_encoded_padded))
